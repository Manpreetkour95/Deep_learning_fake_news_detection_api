{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3baa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62393ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3bce68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n1=['mdxgfcgf','vhjvhjvjhv','bhbjh bjbj','vghv dxdfxfdx',''\n",
    "n1=['fzfrbski', 'jxftxhav', 'soelzlmv', 'exvpgozj', 'utxdmzua', 'ptjrlebl', 'nncetmza', 'lwfyugqc', 'apwqiqnu', 'jplhbrwa', 'ywsdtaff', 'guxnibck', 'lddmxdmn', 'fxgdzwhg', 'teigzvhv', 'xpxxtnhp', 'pzhgsrnu', 'cuhfkgby', 'drtgkgrm', 'alvwhbtu', 'ovnbthzy']\n",
    "\n",
    "    #n2 = ['Emma', 'Olivia', 'Ava', 'Isabella', 'Sophia', 'Mia', 'Charlotte', 'Amelia', 'Evelyn', 'Abigail', 'Harper', 'Emily', 'Elizabeth', 'Avery', 'Sofia', 'Ella', 'Madison', 'Scarlett', 'Victoria', 'Aria', 'Grace', 'Chloe', 'Camila', 'Penelope', 'Riley', 'Layla', 'Lillian', 'Nora', 'Zoey', 'Mila', 'Aubrey', 'Hannah', 'Lily', 'Addison', 'Eleanor', 'Natalie', 'Luna', 'Savannah', 'Brooklyn', 'Leah', 'Zoe', 'Stella', 'Hazel', 'Ellie', 'Paisley', 'Audrey', 'Skylar', 'Violet', 'Claire', 'Bella', 'Aaliyah', 'Lucy', 'Anna', 'Samantha', 'Caroline', 'Genesis', 'Aurora', 'Kennedy', 'Kinsley', 'Maya', 'Allison', 'Sarah', 'Madelyn', 'Adeline', 'Alexa', 'Ariana', 'Elena', 'Gabriella', 'Naomi', 'Alice', 'Sadie', 'Hailey', 'Eva', 'Emilia', 'Autumn', 'Quinn', 'Nevaeh', 'Piper', 'Ruby', 'Serenity', 'Willow', 'Everly', 'Cora', 'Kaylee', 'Lydia', 'Aubree', 'Arianna', 'Eliana', 'Peyton', 'Melanie', 'Gianna', 'Isabelle', 'Julia', 'Valentina', 'Nova', 'Clara', 'Vivian', 'Reagan', 'Mackenzie', 'Madeline', 'Brielle', 'Delilah', 'Isla', 'Rylee', 'Katherine', 'Sophie', 'Josephine', 'Ivy', 'Liliana', 'Jade', 'Maria', 'Taylor', 'Hadley', 'Kylie', 'Emery', 'Adalynn', 'Natalia', 'Annabelle', 'Faith', 'Alexandra', 'Ximena', 'Ashley', 'Brianna', 'Raelynn', 'Bailey', 'Mary', 'Athena', 'Andrea', 'Leilani', 'Jasmine', 'Lyla', 'Margaret', 'Amanda', 'Daniela', 'Jordyn', 'Lia', 'Haley', 'Raegan', 'Camille', 'Hope', 'Makayla', 'Miriam', 'Alicia', 'Mckenna', 'Gabrielle', 'Lyric', 'Kelsey', 'Kinley', 'Adelaide', 'Alexia', 'Alaina', 'Brynn', 'Catalina', 'Kamryn', 'Karina', 'Brittany', 'Daniella', 'Felicity', 'Harmony', 'Julianna', 'Kailey', 'Presley', 'Jayla', 'Journey', 'Elaina', 'Maggie', 'Adalyn', 'Celeste', 'Cecilia', 'Aniyah', 'Amber', 'Emerson', 'Emersyn', 'Marley', 'Kendra', 'Chelsea', 'Makenna', 'Lilah', 'Ayla']\n",
    "c1 = [    'Global',    'International',    'National',    'Regional',    'Local',    'Digital',    'Tech',    'Software',    'Hardware',    'Engineering',    'Solutions',    'Services',    'Systems',    'Technologies',    'Innovations',    'Consulting',    'Group',    'Partners',    'Enterprises',    'Inc.',    'Corporation',    'LLC',    'Association',    'Institute',    'Center',    'Foundation',    'Network',    'Agency',    'Alliance',    'Consortium',    'Enterprise',    'Venture',    'Company',    'Firm',    'Business',    'Holdings',    'Ventures',    'Management',    'Capital',    'Investment',    'Fund',    'Trust',    'Bank',    'Credit',    'Union',    'Insurance',    'Assurance',    'Healthcare',    'Medical',    'Health',    'Pharmaceuticals',    'Biotech',    'Research',    'Development',    'Sciences',    'Engineering',    'Design',    'Architecture',    'Construction',    'Real Estate',    'Property',    'Marketing',    'Advertising',    'Public Relations',    'Communications',    'Media',    'Publishing',    'Education',    'Training',    'Coaching',    'Learning',    'Legal',    'Law',    'Accounting',    'Finance',    'Banking',    'Investment',    'Trading',    'Stock',    'Brokerage',    'Retail',    'Wholesale',    'Manufacturing',    'Distribution',    'Logistics',    'Supply Chain',    'Transport',    'Shipping',    'Freight',    'Import',    'Export',    'Energy',    'Utilities',    'Renewable',    'Environment',    'Sustainability',    'Nonprofit',    'Charity',    'Social']\n",
    "c2= [    'Global',    'International',    'National',    'Regional',    'Local',    'Digital',    'Tech',    'Software',    'Hardware',    'Engineering',    'Solutions',    'Services',    'Systems',    'Technologies',    'Innovations',    'Consulting',    'Group',    'Partners',    'Enterprises',    'Inc.',    'Corporation',    'LLC',    'Association',    'Institute',    'Center',    'Foundation',    'Network',    'Agency',    'Alliance',    'Consortium',    'Enterprise',    'Venture',    'Company',    'Firm',    'Business',    'Holdings',    'Ventures',    'Management',    'Capital',    'Investment',    'Fund',    'Trust',    'Bank',    'Credit',    'Union',    'Insurance',    'Assurance',    'Healthcare',    'Medical',    'Health',    'Pharmaceuticals',    'Biotech',    'Research',    'Development',    'Sciences',    'Engineering',    'Design',    'Architecture',    'Construction',    'Real Estate',    'Property',    'Marketing',    'Advertising',    'Public Relations',    'Communications',    'Media',    'Publishing',    'Education',    'Training',    'Coaching',    'Learning',    'Legal',    'Law',    'Accounting',    'Finance',    'Banking',    'Investment',    'Trading',    'Stock',    'Brokerage',    'Retail',    'Wholesale',    'Manufacturing',    'Distribution',    'Logistics',    'Supply Chain',    'Transport',    'Shipping',    'Freight',    'Import',    'Export',    'Energy',    'Utilities',    'Renewable',    'Environment',    'Sustainability',    'Nonprofit',    'Charity',    'Social',    'Assist',    'Sustain',    'Transform',    'Innovate',    'Advance',    'Discover',    'Evolve',    'Enable',    'Empower',    'Inspire',    'Nurture',    'Pioneer',    'Revolutionize',    'Elevate',    'Optimize',    'Empowerment',    'Progress',    'Impact',    'Change',    'Revolution',    'Revive',    'Modernize',    'Future',    'Tomorrow',    'Today',    'NextGen',    'NextGeneration',    'New',    'Institute',    'Collective',    'Action',    'Initiative',    'Collaborative',    'United',    'Synergy',    'Sustainable',    'Healthy',    'Wellness',    'Wellbeing',    'Peace',    'Hope',    'Trust',    'Freedom',    'Empathy',    'Diversity',    'Equity',    'Inclusion']\n",
    "c=list(set(c1+c2))\n",
    "n=n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b933f3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cuhfkgby</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Revive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fzfrbski</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Supply Chain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Retail</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Company</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ptjrlebl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Logistics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ovnbthzy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Fund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             text  label\n",
       "17       cuhfkgby      0\n",
       "93         Revive      1\n",
       "0        fzfrbski      0\n",
       "28   Supply Chain      1\n",
       "24         Retail      1\n",
       "..            ...    ...\n",
       "111       Company      1\n",
       "5        ptjrlebl      0\n",
       "66      Logistics      1\n",
       "20       ovnbthzy      0\n",
       "88           Fund      1\n",
       "\n",
       "[163 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(n,columns =['text'])\n",
    "df1['label']=0\n",
    "df2 = pd.DataFrame(c,columns =['text'])\n",
    "df2['label']=1\n",
    "df=pd.concat([df1,df2],axis=0)\n",
    "df=df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "af479c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         3\n",
      "          1       1.00      1.00      1.00        30\n",
      "\n",
      "avg / total       1.00      1.00      1.00        33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Load the pre-trained GloVe embeddings\n",
    "word_embeddings = {}\n",
    "with open('glove.6B.50d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = embedding\n",
    "\n",
    "# Load the dataset\n",
    "#df = pd.read_csv('dataset.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Preprocess the text by removing stop words and punctuations\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    cleaned_tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess)\n",
    "\n",
    "# Extract the features from the GloVe embeddings\n",
    "def extract_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    features = []\n",
    "    for token in tokens:\n",
    "        if token in word_embeddings:\n",
    "            features.append(word_embeddings[token])\n",
    "    if len(features) == 0:\n",
    "        features.append(np.zeros((50,)))\n",
    "    avg_embedding = np.mean(np.array(features), axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "X = df['text'].apply(extract_features)\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM classifier on the extracted features\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "clf.fit(np.vstack(X_train), y_train)\n",
    "\n",
    "# Evaluate the performance of the classifier on the testing set\n",
    "y_pred = clf.predict(np.vstack(X_test))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "71bcbf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a sample input text\n",
    "input_text = \" a t enterprices\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1acb6e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text = preprocess(input_text)\n",
    "features = extract_features(processed_text)\n",
    "X_test = np.array(features).reshape(1, -1)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed5ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d0170d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c0fef223044bf08d92f998606f07fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/852 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2aee3daea194148a974d852b584d794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05af787545a44c0d91dded24fb59443d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302b09e1be75490190dcf1294e79ecc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Apple is a I-ORG\n",
      "▁in is a I-ORG\n",
      "c is a I-ORG\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_model = pipeline(\"ner\", model=\"xlm-roberta-large-finetuned-conll03-english\", tokenizer=\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "\n",
    "\n",
    "text = \"Apple inc\"\n",
    "results = ner_model(text)\n",
    "\n",
    "for result in results:\n",
    "    if result['entity'] == 'ORG':\n",
    "        print(result['word'], \"is an organization.\")\n",
    "    elif result['entity'] == 'PER':\n",
    "        print(result['word'], \"is a name.\")\n",
    "    else:\n",
    "        print(result['word'], \"is a\", result['entity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6528c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁mich is a I-PER\n",
      "el is a I-PER\n",
      "▁kam is a I-PER\n",
      "boj is a I-PER\n",
      "▁roma is a I-PER\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"michel kamboj  roma\"\n",
    "results = ner_model(text)\n",
    "\n",
    "for result in results:\n",
    "    if result['entity'] == 'ORG':\n",
    "        print(result['word'], \"is an organization.\")\n",
    "    elif result['entity'] == 'PER':\n",
    "        print(result['word'], \"is a name.\")\n",
    "    else:\n",
    "        print(result['word'], \"is a\", result['entity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "546d3ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-baeb28169cd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlm-roberta-large-finetuned-conll03-english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlm-roberta-large-finetuned-conll03-english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Alya told Jasmine that Andrew could pay with cash..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Alya told Jasmine that Andrew could pay with cash..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4aadca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
